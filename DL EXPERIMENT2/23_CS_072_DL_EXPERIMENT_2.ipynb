{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNowooYhxWjTlr8lak/MhZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apoorvmittal11/23-CS-072-DL-LAB-EXPERIMENT/blob/main/DL%20EXPERIMENT2/23_CS_072_DL_EXPERIMENT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build and train a fully connected neural network , without relying on deep learning libraries such as TensorFlow or PyTorch.\n",
        "\n",
        "\n",
        "Dataset: MNIST\n",
        "\n",
        "You can use the torch library only to load the dataset like this:\n",
        "\n",
        "import torch\n",
        "\n",
        "import torchvision\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "val_dataset = torchvision.datasets.MNIST(\n",
        "\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "define batches\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=64,shuffle=True)\n",
        "\n",
        "val_loader= torch.utils.data.DataLoader(dataset=val_dataset,batch_size=64,shuffle= False)\n",
        "\n",
        "After this, make sure you convert the tensors to numpy, as you will be implementing this in numpy.\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    # images: torch.Tensor of shape (batch_size, 1, 28, 28)\n",
        "    # labels: torch.Tensor of shape (batch_size,)\n",
        "\n",
        "    # Step 1: Move to CPU (important if CUDA is enabled)\n",
        "    images = images.cpu()\n",
        "    labels = labels.cpu()\n",
        "\n",
        "    # Step 2: Convert to NumPy\n",
        "    images_np = images.numpy()\n",
        "    labels_np = labels.numpy()\n",
        "\n",
        "Why .cpu() Is Required\n",
        "* NumPy\n",
        "\n",
        "* Calling .cpu() ensures the tensor is in host memory\n",
        "\n",
        "* Torch is allowed up to .cpu().numpy(); beyond that point, only NumPy is permitted.  "
      ],
      "metadata": {
        "id": "SHaCn1C9zHxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation Requirements**\n",
        "\n",
        "* Load the MNIST dataset\n",
        "* Normalize input pixel values\n",
        "* One-hot encode class labels\n",
        "* Split the training data into and sets\n"
      ],
      "metadata": {
        "id": "p2O1yZUvzHuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "val_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Batch size defined here\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "def process_batch(images, labels):\n",
        "    # Step 1 & 2: CPU and Numpy\n",
        "    images_np = images.cpu().numpy()\n",
        "    labels_np = labels.cpu().numpy()\n",
        "\n",
        "    input_data = images_np.reshape(images_np.shape[0], -1)\n",
        "\n",
        "    # One-hot encode\n",
        "    one_hot_labels = np.zeros((labels_np.size, 10))\n",
        "\n",
        "    one_hot_labels[np.arange(labels_np.size), labels_np] = 1\n",
        "\n",
        "    return input_data, one_hot_labels, labels_np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8aPqRtL0lZC",
        "outputId": "151ae975-cac5-4007-de6c-ce1f8e85887a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 42.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.19MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.2MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.88MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inside the NeuralNetwork class, implement the following methods:\n",
        "\n",
        "1.)forward()\n",
        "* Perform forward propagation through all layers\n",
        "* Compute and store intermediate activations\n",
        "\n",
        "\n",
        "2.)backward()\n",
        "* Perform backpropagation\n",
        "* Compute gradients of weights and biases\n",
        "\n",
        "\n",
        "3.)compute_loss()\n",
        "* Calculate the loss (e.g., Cross-Entropy Loss )\n",
        "\n",
        "\n",
        "4.)update_parameters()\n",
        "* Update weights and biases using gradient descent\n",
        "* predict() for inference\n",
        "* evaluate() to compute accuracy."
      ],
      "metadata": {
        "id": "4gJSUwa6zHr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activations:\n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        return np.maximum(0, z)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_deriv(z):\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_deriv(z):\n",
        "        s = Activations.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(z):\n",
        "        return np.tanh(z)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_deriv(z):\n",
        "        return 1 - np.tanh(z)**2\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(z):\n",
        "        exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "vEf2Intn1iFY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes, activation='relu', learning_rate=0.01):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.params = {}\n",
        "        self.act_func_name = activation\n",
        "\n",
        "        np.random.seed(42)\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            input_dim = layer_sizes[i-1]\n",
        "            output_dim = layer_sizes[i]\n",
        "\n",
        "            if activation == 'relu':\n",
        "                scale = np.sqrt(2.0 / input_dim)\n",
        "            else:\n",
        "                scale = np.sqrt(1.0 / input_dim)\n",
        "\n",
        "            self.params['W' + str(i)] = np.random.randn(input_dim, output_dim) * scale\n",
        "            self.params['b' + str(i)] = np.zeros((1, output_dim))\n",
        "\n",
        "    def _get_activation(self, z):\n",
        "        if self.act_func_name == 'relu':\n",
        "            return Activations.relu(z)\n",
        "        elif self.act_func_name == 'sigmoid':\n",
        "            return Activations.sigmoid(z)\n",
        "        elif self.act_func_name == 'tanh':\n",
        "            return Activations.tanh(z)\n",
        "        return z\n",
        "\n",
        "    def _get_activation_deriv(self, z):\n",
        "        if self.act_func_name == 'relu':\n",
        "            return Activations.relu_deriv(z)\n",
        "        elif self.act_func_name == 'sigmoid':\n",
        "            return Activations.sigmoid_deriv(z)\n",
        "        elif self.act_func_name == 'tanh':\n",
        "            return Activations.tanh_deriv(z)\n",
        "        return 1\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.cache = {'A0': X}\n",
        "        L = len(self.layer_sizes) - 1\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, L):\n",
        "            Z = np.dot(self.cache['A' + str(i-1)], self.params['W' + str(i)]) + self.params['b' + str(i)]\n",
        "            A = self._get_activation(Z)\n",
        "            self.cache['Z' + str(i)] = Z\n",
        "            self.cache['A' + str(i)] = A\n",
        "\n",
        "        # Output layer\n",
        "        Z_out = np.dot(self.cache['A' + str(L-1)], self.params['W' + str(L)]) + self.params['b' + str(L)]\n",
        "        A_out = Activations.softmax(Z_out)\n",
        "\n",
        "        self.cache['Z' + str(L)] = Z_out\n",
        "        self.cache['A' + str(L)] = A_out\n",
        "\n",
        "        return A_out\n",
        "\n",
        "    def compute_loss(self, Y_hat, Y):\n",
        "        m = Y.shape[0]\n",
        "        # Add epsilon to prevent log(0)\n",
        "        epsilon = 1e-15\n",
        "        cost = -np.sum(Y * np.log(Y_hat + epsilon)) / m\n",
        "        return cost\n",
        "\n",
        "    def backward(self, Y, Y_hat):\n",
        "        grads = {}\n",
        "        L = len(self.layer_sizes) - 1\n",
        "        m = Y.shape[0]\n",
        "\n",
        "        dZ = Y_hat - Y\n",
        "\n",
        "        grads['dW' + str(L)] = np.dot(self.cache['A' + str(L-1)].T, dZ) / m\n",
        "        grads['db' + str(L)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "        for i in range(L-1, 0, -1):\n",
        "            dA = np.dot(dZ, self.params['W' + str(i+1)].T)\n",
        "            dZ = dA * self._get_activation_deriv(self.cache['Z' + str(i)])\n",
        "\n",
        "            grads['dW' + str(i)] = np.dot(self.cache['A' + str(i-1)].T, dZ) / m\n",
        "            grads['db' + str(i)] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_parameters(self, grads):\n",
        "        L = len(self.layer_sizes) - 1\n",
        "        for i in range(1, L + 1):\n",
        "            self.params['W' + str(i)] -= self.learning_rate * grads['dW' + str(i)]\n",
        "            self.params['b' + str(i)] -= self.learning_rate * grads['db' + str(i)]\n",
        "\n",
        "    def predict(self, X):\n",
        "        A_out = self.forward(X)\n",
        "        return np.argmax(A_out, axis=1)\n",
        "\n",
        "    def evaluate(self, X, Y_true):\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = np.mean(predictions == Y_true)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "A4Va31cy1mhf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A.Train the model for multiple epochs\n",
        "\n",
        "B.Compute and record:\n",
        "* Training loss\n",
        "* Training accuracy\n",
        "* Validation loss\n",
        "* Validation accuracy per epoch"
      ],
      "metadata": {
        "id": "9THaIsHczHpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            X_batch, Y_batch_onehot, Y_batch_labels = process_batch(images, labels)\n",
        "\n",
        "            Y_hat = model.forward(X_batch)\n",
        "\n",
        "            loss = model.compute_loss(Y_hat, Y_batch_onehot)\n",
        "            epoch_loss += loss\n",
        "\n",
        "            grads = model.backward(Y_batch_onehot, Y_hat)\n",
        "\n",
        "            model.update_parameters(grads)\n",
        "\n",
        "            preds = np.argmax(Y_hat, axis=1)\n",
        "            correct_train += np.sum(preds == Y_batch_labels)\n",
        "            total_train += Y_batch_labels.shape[0]\n",
        "\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        train_acc = correct_train / total_train\n",
        "\n",
        "        val_loss_accum = 0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        for images, labels in val_loader:\n",
        "            X_val, Y_val_onehot, Y_val_labels = process_batch(images, labels)\n",
        "\n",
        "            Y_hat_val = model.forward(X_val)\n",
        "\n",
        "            val_loss_accum += model.compute_loss(Y_hat_val, Y_val_onehot)\n",
        "\n",
        "            preds_val = np.argmax(Y_hat_val, axis=1)\n",
        "            correct_val += np.sum(preds_val == Y_val_labels)\n",
        "            total_val += Y_val_labels.shape[0]\n",
        "\n",
        "        avg_val_loss = val_loss_accum / len(val_loader)\n",
        "        val_acc = correct_val / total_val\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "model = NeuralNetwork(layer_sizes=[784, 128, 10], activation='relu', learning_rate=0.1)\n",
        "\n",
        "history = train_model(model, train_loader, val_loader, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9458g0s2RTh",
        "outputId": "1fee9767-f6cb-48d5-bb25-5257d9d26447"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train Loss: 0.3758 | Train Acc: 0.8936 | Val Loss: 0.2358 | Val Acc: 0.9306\n",
            "Epoch 2/5 | Train Loss: 0.1996 | Train Acc: 0.9433 | Val Loss: 0.2018 | Val Acc: 0.9392\n",
            "Epoch 3/5 | Train Loss: 0.1487 | Train Acc: 0.9577 | Val Loss: 0.1336 | Val Acc: 0.9588\n",
            "Epoch 4/5 | Train Loss: 0.1199 | Train Acc: 0.9661 | Val Loss: 0.1105 | Val Acc: 0.9675\n",
            "Epoch 5/5 | Train Loss: 0.1010 | Train Acc: 0.9715 | Val Loss: 0.1008 | Val Acc: 0.9713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiments-**\n",
        "\n",
        "Try out different hyperparameter configurations and log the results for each experiment. Since these results will be included in your submission files under the experiment section, ensure that all tables and plots are properly saved:\n",
        "\n",
        "* Number of hidden layers\n",
        "* Number of neurons per layer\n",
        "* Activation functions (ReLU, Sigmoid, Tanh)\n",
        "\n",
        "Use activation in the output layer"
      ],
      "metadata": {
        "id": "zBcbXL2lzHmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph1BqNvtzEPB",
        "outputId": "75e53a96-9a35-4b68-cd6f-b5716ff6c9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: ReLU, 1 Hidden Layer \n",
            "Epoch 1/5 | Train Loss: 0.3753 | Train Acc: 0.8950 | Val Loss: 0.2223 | Val Acc: 0.9349\n",
            "Epoch 2/5 | Train Loss: 0.1987 | Train Acc: 0.9433 | Val Loss: 0.1593 | Val Acc: 0.9533\n",
            "Epoch 3/5 | Train Loss: 0.1489 | Train Acc: 0.9572 | Val Loss: 0.1366 | Val Acc: 0.9595\n",
            "Epoch 4/5 | Train Loss: 0.1192 | Train Acc: 0.9669 | Val Loss: 0.1076 | Val Acc: 0.9675\n",
            "Epoch 5/5 | Train Loss: 0.1006 | Train Acc: 0.9718 | Val Loss: 0.0947 | Val Acc: 0.9723\n",
            "\n",
            "2: Tanh, 2 Hidden Layers \n",
            "Epoch 1/5 | Train Loss: 0.3869 | Train Acc: 0.8938 | Val Loss: 0.2502 | Val Acc: 0.9297\n",
            "Epoch 2/5 | Train Loss: 0.2085 | Train Acc: 0.9392 | Val Loss: 0.1887 | Val Acc: 0.9415\n",
            "Epoch 3/5 | Train Loss: 0.1535 | Train Acc: 0.9558 | Val Loss: 0.1315 | Val Acc: 0.9607\n",
            "Epoch 4/5 | Train Loss: 0.1223 | Train Acc: 0.9641 | Val Loss: 0.1226 | Val Acc: 0.9624\n",
            "Epoch 5/5 | Train Loss: 0.1017 | Train Acc: 0.9698 | Val Loss: 0.1066 | Val Acc: 0.9659\n",
            "\n",
            "3: Sigmoid, 1 Hidden Layer \n",
            "Epoch 1/5 | Train Loss: 0.7823 | Train Acc: 0.8074 | Val Loss: 0.3924 | Val Acc: 0.8914\n",
            "Epoch 2/5 | Train Loss: 0.3600 | Train Acc: 0.8994 | Val Loss: 0.3124 | Val Acc: 0.9121\n",
            "Epoch 3/5 | Train Loss: 0.3109 | Train Acc: 0.9101 | Val Loss: 0.2839 | Val Acc: 0.9201\n",
            "Epoch 4/5 | Train Loss: 0.2840 | Train Acc: 0.9180 | Val Loss: 0.2616 | Val Acc: 0.9258\n",
            "Epoch 5/5 | Train Loss: 0.2633 | Train Acc: 0.9240 | Val Loss: 0.2475 | Val Acc: 0.9292\n"
          ]
        }
      ],
      "source": [
        "print(\"1: ReLU, 1 Hidden Layer \")\n",
        "model_1 = NeuralNetwork(layer_sizes=[784, 128, 10], activation='relu', learning_rate=0.1)\n",
        "history_1 = train_model(model_1, train_loader, val_loader, epochs=5)\n",
        "\n",
        "print(\"\\n2: Tanh, 2 Hidden Layers \")\n",
        "model_2 = NeuralNetwork(layer_sizes=[784, 128, 64, 10], activation='tanh', learning_rate=0.1)\n",
        "history_2 = train_model(model_2, train_loader, val_loader, epochs=5)\n",
        "\n",
        "print(\"\\n3: Sigmoid, 1 Hidden Layer \")\n",
        "model_3 = NeuralNetwork(layer_sizes=[784, 128, 10], activation='sigmoid', learning_rate=0.1)\n",
        "history_3 = train_model(model_3, train_loader, val_loader, epochs=5)"
      ]
    }
  ]
}