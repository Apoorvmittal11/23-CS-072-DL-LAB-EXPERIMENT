{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZDzr51tD+ukZhPFZ19fdh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Text Generation using RNN and LSTM**"
      ],
      "metadata": {
        "id": "7ZBMZjdZ7eGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 Objective**\n",
        "\n",
        "The aim of this experiment is to explore text generation using Recurrent Neural\n",
        "Networks (RNNs) and understand the impact of different word representations:\n",
        "1. One-Hot Encoding\n",
        "2. Trainable Word Embeddings\n",
        "\n",
        "Train an RNN model on a dataset of 100 poems and compare the perfor-\n",
        "mance of both encoding techniques."
      ],
      "metadata": {
        "id": "fjGXxelt7d74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 Dataset**\n",
        "\n",
        "Use the provided dataset of 100 poems for training your text generation model.\n",
        "The dataset consists of multiple lines of poetry, which will be used to generate\n",
        "text sequences."
      ],
      "metadata": {
        "id": "Qb_bsgAa7d4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Load Data\n",
        "print(\"Loading data...\")\n",
        "try:\n",
        "    df = pd.read_csv('poems-100.csv')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Please upload 'poems-100.csv' to your Colab files.\")\n",
        "\n",
        "# 2. Preprocessing\n",
        "# Combine all poems into one long string\n",
        "corpus = \" \".join(df['text'].astype(str).tolist()).lower()\n",
        "words = corpus.split()\n",
        "\n",
        "# Build Vocabulary\n",
        "word_counts = Counter(words)\n",
        "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create Mappings (Word -> Index and Index -> Word)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "print(f\"Total Words: {len(words)}\")\n",
        "print(f\"Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# 3. Create Sequences for Training\n",
        "seq_length = 5\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(words) - seq_length):\n",
        "    seq_in = words[i:i + seq_length]\n",
        "    seq_out = words[i + seq_length]\n",
        "    X_data.append([word_to_idx[w] for w in seq_in])\n",
        "    y_data.append(word_to_idx[seq_out])\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_data, dtype=torch.long)\n",
        "y_tensor = torch.tensor(y_data, dtype=torch.long)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Setup complete. Running on: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlC7-fejBg-X",
        "outputId": "68db0b76-0f5d-41fd-c747-a57752e97249"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Dataset loaded successfully.\n",
            "Total Words: 24734\n",
            "Vocabulary Size: 6989\n",
            "Setup complete. Running on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3 Part 1: One-Hot Encoding Approach**"
      ],
      "metadata": {
        "id": "xyQ9-evs7d1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Preprocessing**\n",
        "\n",
        "• Tokenize the text into words.\n",
        "\n",
        "• Convert each word into a one-hot vector.\n",
        "\n",
        "**3.2 Model Architecture**\n",
        "\n",
        "• Use an RNN and LSTM model.\n",
        "\n",
        "• The input should be one-hot encoded word sequences.\n",
        "\n",
        "• Train the model to predict the next word in a sequence.\n",
        "\n",
        "**3.3 Implementation Steps**\n",
        "\n",
        "• Tokenize the dataset and create a vocabulary.\n",
        "\n",
        "• Convert words into one-hot encoded vectors.\n",
        "\n",
        "• Define an RNN model using PyTorch.\n",
        "\n",
        "• Train the model using the dataset.\n",
        "\n",
        "• Generate text using the trained model."
      ],
      "metadata": {
        "id": "Oe1UT8A87dxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on: {device}\")\n",
        "\n",
        "def train_model(model, X, y, epochs=20, lr=0.01, is_one_hot=False, batch_size=64):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Create a DataLoader to handle batching\n",
        "    dataset = TensorDataset(X, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Move batch to device\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if is_one_hot:\n",
        "                # Convert ONLY this small batch to One-Hot\n",
        "                # This saves RAM by not converting the whole dataset at once\n",
        "                batch_X_enc = torch.nn.functional.one_hot(batch_X, num_classes=vocab_size).float()\n",
        "            else:\n",
        "                batch_X_enc = batch_X\n",
        "\n",
        "            output, hidden = model(batch_X_enc)\n",
        "\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_text(model, start_text, length=10, is_one_hot=False):\n",
        "    model.eval()\n",
        "    words_in = start_text.lower().split()\n",
        "    current_seq = [word_to_idx.get(w, 0) for w in words_in]\n",
        "\n",
        "    # Pad or truncate\n",
        "    if len(current_seq) < seq_length:\n",
        "        current_seq = [0]*(seq_length - len(current_seq)) + current_seq\n",
        "    else:\n",
        "        current_seq = current_seq[-seq_length:]\n",
        "\n",
        "    generated_text = list(words_in)\n",
        "\n",
        "    for _ in range(length):\n",
        "        input_tensor = torch.tensor([current_seq], dtype=torch.long).to(device)\n",
        "\n",
        "        if is_one_hot:\n",
        "            input_tensor = torch.nn.functional.one_hot(input_tensor, num_classes=vocab_size).float()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, _ = model(input_tensor)\n",
        "\n",
        "        predicted_idx = torch.argmax(output, dim=1).item()\n",
        "        predicted_word = idx_to_word[predicted_idx]\n",
        "\n",
        "        generated_text.append(predicted_word)\n",
        "        current_seq.append(predicted_idx)\n",
        "        current_seq = current_seq[1:]\n",
        "\n",
        "    return \" \".join(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsyYpIfYEE4r",
        "outputId": "484699ed-d790-4cd4-8c90-265d0dfdaa0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Definitions: One-Hot ---\n",
        "class RNN_OneHot(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, output_size):\n",
        "        super(RNN_OneHot, self).__init__()\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, hidden = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, hidden\n",
        "\n",
        "class LSTM_OneHot(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, output_size):\n",
        "        super(LSTM_OneHot, self).__init__()\n",
        "        self.lstm = nn.LSTM(vocab_size, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hidden, cell) = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, hidden\n",
        "\n",
        "# --- Training (With Batching) ---\n",
        "HIDDEN_DIM = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "print(\"Training RNN (One-Hot)...\")\n",
        "rnn_oh = RNN_OneHot(vocab_size, HIDDEN_DIM, vocab_size)\n",
        "rnn_oh = train_model(rnn_oh, X_tensor, y_tensor, epochs=EPOCHS, is_one_hot=True)\n",
        "\n",
        "print(\"\\nTraining LSTM (One-Hot)...\")\n",
        "lstm_oh = LSTM_OneHot(vocab_size, HIDDEN_DIM, vocab_size)\n",
        "lstm_oh = train_model(lstm_oh, X_tensor, y_tensor, epochs=EPOCHS, is_one_hot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw8S66DyAn6E",
        "outputId": "c6a7a7c0-c894-4a04-da42-6c64c10f917c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN (One-Hot)...\n",
            "Epoch 5/10, Loss: 9.1683\n",
            "Epoch 10/10, Loss: 8.8586\n",
            "\n",
            "Training LSTM (One-Hot)...\n",
            "Epoch 5/10, Loss: 0.9805\n",
            "Epoch 10/10, Loss: 0.0191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4 Part 2: Trainable Word Embeddings Approach**"
      ],
      "metadata": {
        "id": "E7FFPvsr7dun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 Preprocessing**\n",
        "\n",
        "• Tokenize the text into words.\n",
        "\n",
        "• Convert each word into an index.\n",
        "\n",
        "**4.2 Model Architecture**\n",
        "\n",
        "• Use an embedding layer in the RNN model.\n",
        "\n",
        "• Train the embedding layer along with the model.\n",
        "\n",
        "• Predict the next word in a sequence.\n",
        "\n",
        "**4.3 Implementation Steps**\n",
        "\n",
        "1. Tokenize the dataset and create a vocabulary.\n",
        "\n",
        "2. Convert words into indexed sequences.\n",
        "\n",
        "3. Define an RNN model with an embedding layer using PyTorch.\n",
        "\n",
        "4. Train the model and compare performance with the one-hot encoding\n",
        "method.\n",
        "\n",
        "5. Generate text using the trained model."
      ],
      "metadata": {
        "id": "MluaJE18AJpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Definitions: Embeddings ---\n",
        "class RNN_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_size):\n",
        "        super(RNN_Embedding, self).__init__()\n",
        "        # Embedding layer converts integer indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        out, hidden = self.rnn(embeds)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, hidden\n",
        "\n",
        "class LSTM_Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_size):\n",
        "        super(LSTM_Embedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        out, (hidden, cell) = self.lstm(embeds)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, hidden\n",
        "\n",
        "# --- Training ---\n",
        "EMBED_DIM = 50\n",
        "\n",
        "print(\"Training RNN (Embeddings)...\")\n",
        "rnn_emb = RNN_Embedding(vocab_size, EMBED_DIM, HIDDEN_DIM, vocab_size)\n",
        "rnn_emb = train_model(rnn_emb, X_tensor, y_tensor, epochs=20, is_one_hot=False)\n",
        "\n",
        "print(\"\\nTraining LSTM (Embeddings)...\")\n",
        "lstm_emb = LSTM_Embedding(vocab_size, EMBED_DIM, HIDDEN_DIM, vocab_size)\n",
        "lstm_emb = train_model(lstm_emb, X_tensor, y_tensor, epochs=20, is_one_hot=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNQvI981AnMO",
        "outputId": "a165775e-2a6d-4ea8-9c0a-9861da3359c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RNN (Embeddings)...\n",
            "Epoch 5/20, Loss: 6.0397\n",
            "Epoch 10/20, Loss: 4.6417\n",
            "Epoch 15/20, Loss: 4.4079\n",
            "Epoch 20/20, Loss: 4.1944\n",
            "\n",
            "Training LSTM (Embeddings)...\n",
            "Epoch 5/20, Loss: 2.4425\n",
            "Epoch 10/20, Loss: 0.5441\n",
            "Epoch 15/20, Loss: 0.4981\n",
            "Epoch 20/20, Loss: 0.3876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5 Comparison and Analysis**\n",
        "\n",
        "• Compare the training time and loss for both methods.\n",
        "\n",
        "• Evaluate the quality of generated text.\n",
        "\n",
        "• Discuss the advantages and disadvantages of each approach."
      ],
      "metadata": {
        "id": "u_p012SgAJnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALKL97Xv7bx1",
        "outputId": "2358c762-a2f9-4bc6-f7f3-dee56a60a4d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed Text: 'hello india'\n",
            "\n",
            "--------------------------------------------------\n",
            "RNN (One-Hot):     hello india natural natural natural natural natural natural natural natural natural natural\n",
            "LSTM (One-Hot):    hello india pleasure and the printing-office boy? the well-taken photographs—but your wife\n",
            "--------------------------------------------------\n",
            "RNN (Embeddings):  hello india that’s boy the first, not then had did lovely heat\n",
            "LSTM (Embeddings): hello india winds blow round the make toward the foot of the\n"
          ]
        }
      ],
      "source": [
        "# Test Prompt\n",
        "seed_text = \"hello india\"\n",
        "\n",
        "print(f\"Seed Text: '{seed_text}'\\n\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Generate text with One-Hot models\n",
        "print(\"RNN (One-Hot):    \", generate_text(rnn_oh, seed_text, is_one_hot=True))\n",
        "print(\"LSTM (One-Hot):   \", generate_text(lstm_oh, seed_text, is_one_hot=True))\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Generate text with Embedding models\n",
        "print(\"RNN (Embeddings): \", generate_text(rnn_emb, seed_text, is_one_hot=False))\n",
        "print(\"LSTM (Embeddings):\", generate_text(lstm_emb, seed_text, is_one_hot=False))"
      ]
    }
  ]
}